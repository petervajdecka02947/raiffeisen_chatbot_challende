{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9370097-a055-41ca-bc61-71e1b1f44560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain\n",
    "#!pip install pinecone-client\n",
    "#!pip install openai\n",
    "#!pip install matplotlib\n",
    "#%pip install --upgrade tiktoken\n",
    "#%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee64cc99-8f10-41b9-867c-ae18a7c91d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your environment variables as a dictionary\n",
    "# env_variables = {\n",
    "#     'OPENAI_API_KEY': OPENAI_API_KEY,\n",
    "#     'PINECONE_API_KEY': PINECONE_API_KEY,\n",
    "#     'PINECONE_ENV': PINECONE_ENV,\n",
    "#     # Add more variables as needed\n",
    "# }\n",
    "\n",
    "# # Write the environment variables to .env file\n",
    "# with open('.env', 'w') as f:\n",
    "#     for key, value in env_variables.items():\n",
    "#         f.write(f'{key}={value}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8afa5-c374-4a3b-9519-29ad36e0afdd",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fffafcb-0a99-4333-95b6-c69c14b03e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "#from langchain.llms import OpenAI\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "#from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3344f6d0-f614-4334-a11c-3942bc4cb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY= \n",
    "PINECONE_API_KEY=\n",
    "PINECONE_ENV= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34610a0a-cba4-4475-9a07-b044ea40c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "from pydantic_settings import BaseSettings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Take environment variables from .env.\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    OPENAI_API_KEY: str = os.getenv(\"OPENAI_API_KEY\", \"default_openai_api_key\")\n",
    "    PINECONE_API_KEY: str = os.getenv(\"PINECONE_API_KEY\", \"default_pinecone_api_key\")\n",
    "    PINECONE_ENV: str = os.getenv(\"PINECONE_ENV\", \"gcp-starter\")\n",
    "    INDEX_NAME: str = os.getenv(\"INDEX_NAME\", \"brainsoft\")\n",
    "    EMBEDDING_NAME: str = os.getenv(\"EMBEDDING_NAME\", \"default_ambeddings\")\n",
    "    LLM_NAME: str = os.getenv(\"LLM_NAME\", \"default_LLM\")\n",
    "    MONGO_DB_KEY: str = os.getenv(\"MONGO_DB_KEY\", \"default_MONGO_DB_KEY\")\n",
    "\n",
    "\n",
    "# Instantiate settings to be imported by other modules\n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e692cd-2d6f-4a3b-bffd-fba38f9f0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "# Handling class\n",
    "class UpdateError(Exception):\n",
    "    \"\"\"\n",
    "    A custom exception class for handling update-related errors.\n",
    "\n",
    "    This class extends the base Exception class and is used to raise exceptions specifically related to update processes in the application, such as dat\n",
    "\n",
    "    Args:\n",
    "        message (str): The error message describing what went wrong during the update process.\n",
    "        status_code (int): The HTTP status code associated with this error, indicating the nature of the error.\n",
    "\n",
    "    Upon initialization, this class logs the error message using the standard logging module.\n",
    "\n",
    "    Attributes:fv\n",
    "        message (str): Stores the error message.\n",
    "        status_code (int): Stores the HTTP status code associated with the error.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, message, status_code):\n",
    "        super().__init__(message)  # Initialize the base Exception class first\n",
    "        self.message = message\n",
    "        self.status_code = status_code\n",
    "        self.log_error()  # Log the error after initializing\n",
    "\n",
    "    def log_error(self):\n",
    "        logging.error(self.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e5c0fdc-b825-4441-ac18-c428869558b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "#from config import settings\n",
    "import pinecone\n",
    "import openai\n",
    "# from utils.callback_handler_agent import *\n",
    "# from utils.callback_handler_chain import *\n",
    "# from utils.error_handler import *\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.tools import DuckDuckGoSearchRun, Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "def setup_conversational_chain(settings: object):\n",
    "    \"\"\"\n",
    "    Initializes the conversational chain with various tools and configurations.\n",
    "\n",
    "    Args:\n",
    "        settings (object): Application settings containing configuration details.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the agent, retriever, and language model chain.\n",
    "\n",
    "    This function sets up the conversational agent with various tools (like retrievers and search functions)\n",
    "    and configures the language model chain. It handles the initialization of the database, vector database,\n",
    "    language model, and other components required for the conversational chain.\n",
    "\n",
    "    Raises:\n",
    "        UpdateError: If there is an error during the initialization of any component.\n",
    "    \"\"\"\n",
    "    global agent\n",
    "    global retriever\n",
    "    global llm\n",
    "\n",
    "    tools = []\n",
    "\n",
    "    # Initialize database\n",
    "\n",
    "    try:\n",
    "        pinecone.init(\n",
    "            api_key=settings.PINECONE_API_KEY, environment=settings.PINECONE_ENV\n",
    "        )\n",
    "\n",
    "        embeddings_model = OpenAIEmbeddings(\n",
    "            model=settings.EMBEDDING_NAME, openai_api_key=settings.OPENAI_API_KEY\n",
    "        )\n",
    "\n",
    "        vectordb = Pinecone.from_existing_index(settings.INDEX_NAME, embeddings_model)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise UpdateError(f\"Error during initialization of vector database: {e}\", 401)\n",
    "\n",
    "    # Initialize database LLM model\n",
    "\n",
    "    try:\n",
    "        llm = ChatOpenAI(\n",
    "            openai_api_key=settings.OPENAI_API_KEY,\n",
    "            model_name=settings.LLM_NAME,\n",
    "            temperature=0,\n",
    "            streaming=True,\n",
    "            callbacks=[StreamingStdOutCallbackHandler()],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise UpdateError(f\"Error during initialization of LLM: {e}\", 402)\n",
    "    # Prepare retriever\n",
    "\n",
    "    try:\n",
    "        retriever = vectordb.as_retriever(\n",
    "            search_type=\"similarity_score_threshold\",\n",
    "            search_kwargs={\"score_threshold\": 0.1}, # , \"k\": 1\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise UpdateError(f\"Error during initialization of retriever: {e}\", 403)\n",
    "\n",
    "    # Initialize tools\n",
    "\n",
    "    try:\n",
    "        tool_retrieve = create_retriever_tool(\n",
    "            retriever,\n",
    "            \"product_search\",\n",
    "            \"Searches and returns products regarding the pinterest fashion that meets all requirements (if provided) such as age, gender, location, brand, price, availability. Focus on high rating of products first and click_rate second!\",\n",
    "        )\n",
    "\n",
    "        search = DuckDuckGoSearchRun()\n",
    "        search_tool = Tool(\n",
    "            name=\"DuckDuckGo\",\n",
    "            func=search,  # .run\n",
    "            description=\"This tool is used when you need to do a search on the internet to find information that another tool product_search can't find.\",\n",
    "        )\n",
    "\n",
    "        tools.append(tool_retrieve)\n",
    "        tools.append(search_tool)\n",
    "\n",
    "        #        Initialize tools\n",
    "        agent = initialize_agent(\n",
    "            agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, <-\n",
    "            tools=tools,\n",
    "            llm=llm,\n",
    "            verbose=True,\n",
    "            max_iterations=10,\n",
    "            early_stopping_method=\"generate\",\n",
    "            # memory=memory,\n",
    "            return_intermediate_steps=False,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise UpdateError(f\"Error during initialization of toolls an agent: {e}\", 404)\n",
    "    # Set template for history input\n",
    "\n",
    "    try:\n",
    "        new_string = \"\\n\\n\\nCHAT HISTORY AND USER'S INPUT\\n-----------------------------\\nHere is the Chat history followed by user's input\"\n",
    "        old_string = (\n",
    "            \"\\n```\\n\\nUSER'S INPUT\\n--------------------\\nHere is the user's input\"\n",
    "        )\n",
    "        template_new = agent.agent.llm_chain.prompt.messages[2].prompt.template.replace(\n",
    "            old_string, new_string\n",
    "        )\n",
    "\n",
    "        agent.agent.llm_chain.prompt.messages[2].prompt.template = template_new\n",
    "\n",
    "    except Exception as e:\n",
    "        raise UpdateError(f\"Unable to set custom template: {e}\", 405)\n",
    "\n",
    "    try:\n",
    "        template = (\n",
    "            \"\"\"The system should generate a natural language response recommending products with justifications\"\"\"\n",
    "            \"\"\"(e.g., ”Based on your [requirements], we recommend product X from brand Y because it\"\"\"\n",
    "            \"\"\"has a high rating and is within your budget”), ensuring it is fully\"\"\"\n",
    "            \"\"\"supported by the provided context and make sure each product from context must be\"\"\"\n",
    "            \"\"\"mentioned! If any product doesnt meet any requirement, mention it. For example, selected product\"\"\"\n",
    "            \"\"\" is a bit more expensive but it fits your other requirements perfectly!\n",
    "        Context: {context}\n",
    "        Question:{input}\n",
    "        Answer: \"\"\"\n",
    "        )\n",
    "\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"input\", \"context\"])\n",
    "\n",
    "        llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise UpdateError(f\"Unable to set OpenAI chain: {e}\", 406)\n",
    "    return agent, retriever, llm_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dddb6db8-c858-4484-a930-958de756d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent, retriever, llm = setup_conversational_chain(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee5da246-eda2-44dd-a157-c21e9b46c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Recomment shoes for male 800 in Slovakia  but my butget is not more than 80 dolars!\"\n",
    "#query=\"What is IBM ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke({\"input\": query,\"chat_history\":[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ab73a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
